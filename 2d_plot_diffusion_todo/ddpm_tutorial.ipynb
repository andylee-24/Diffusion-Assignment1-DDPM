{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec9392c-4965-4ed4-893d-83e6ed235d57",
   "metadata": {},
   "source": [
    "# Visualize target and prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcd84f-9327-4b0d-b541-be9c0774044c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import TwoDimDataClass, get_data_iterator\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, Output\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "target_ds = TwoDimDataClass(dataset_type='swiss_roll', \n",
    "                            N=1000000, \n",
    "                            batch_size=256)\n",
    "\n",
    "prior_ds = TwoDimDataClass(dataset_type='gaussian_centered',\n",
    "                           N=1000000,\n",
    "                           batch_size=256)\n",
    "\n",
    "num_vis_particles = 500\n",
    "sample_f = target_ds[0:num_vis_particles]\n",
    "sample_b = prior_ds[0:num_vis_particles]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(sample_f[:, 0], sample_f[:, 1], alpha=0.6)\n",
    "ax.scatter(sample_b[:, 0], sample_b[:, 1], alpha=0.6)\n",
    "ax.grid(False)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "strtitle = \"Target and prior distributions\"\n",
    "ax.set_title(strtitle)\n",
    "ax.legend(['Target', 'Prior (=$\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2a4c1-8ec3-436e-b1fd-4a33581af2a2",
   "metadata": {},
   "source": [
    "# Define a diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3d90a-51b4-4da2-ac2e-31e2a9e5900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import SimpleNet\n",
    "from ddpm import BaseScheduler, DiffusionModule\n",
    "\n",
    "# hyperparameters #\n",
    "# Don't change it\n",
    "device = \"cuda:0\"\n",
    "config = {\n",
    "    \"num_diffusion_steps\": 1000,\n",
    "    \"dim_hids\": [128, 128, 128],\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_train_iters\": 5000,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "###################\n",
    "\n",
    "def build_ddpm(config):\n",
    "    network = SimpleNet(dim_in=2, \n",
    "                        dim_out=2, \n",
    "                        dim_hids=config[\"dim_hids\"], \n",
    "                        num_timesteps=config[\"num_diffusion_steps\"]\n",
    "                       )\n",
    "    var_scheduler = BaseScheduler(config[\"num_diffusion_steps\"])\n",
    "\n",
    "    ddpm = DiffusionModule(network, var_scheduler).to(config[\"device\"])\n",
    "    return ddpm\n",
    "\n",
    "ddpm = build_ddpm(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dab7b8-bef4-4186-84b8-2a42f1dff8e4",
   "metadata": {},
   "source": [
    "# Visualize q(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b4dad-61fa-487f-a6f1-98eb65b83971",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(28, 3))\n",
    "for i, t in enumerate(range(0, 500, 50)):\n",
    "    x_t = ddpm.q_sample(target_ds[:num_vis_particles].to(device), (torch.ones(num_vis_particles) * t).to(device))\n",
    "    x_t = x_t.cpu()\n",
    "    axs[i].scatter(x_t[:,0], x_t[:,1], color='white',edgecolor='gray', s=5)\n",
    "    axs[i].set_axis_off()\n",
    "    axs[i].set_title('$q(\\mathbf{x}_{'+str(t)+'})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d91bf8-0ccc-467f-8f56-90a30c7c352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should be able to see the distributions as below.\n",
    "vis_qs = Image.open(\"../assets/images/qs.png\")\n",
    "display(vis_qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4341536-6b05-4431-a783-5ac4098af622",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b17dc2-bfe8-4f24-84a1-5e8803f0849e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def figure2image(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img\n",
    "\n",
    "# Initialize the model.\n",
    "ddpm = build_ddpm(config)\n",
    "\n",
    "pbar = tqdm(range(config[\"num_train_iters\"]))\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=config[\"lr\"])\n",
    "train_dl = torch.utils.data.DataLoader(target_ds, batch_size=config[\"batch_size\"])\n",
    "train_iter = get_data_iterator(train_dl)\n",
    "\n",
    "losses = []\n",
    "images = []\n",
    "try:\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_x = next(train_iter)\n",
    "        batch_x = batch_x.to(device)\n",
    "        loss = ddpm.compute_loss(batch_x)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 4999 == 0:\n",
    "            with torch.no_grad():\n",
    "                ####\n",
    "                # NOTE: If you haven't implemented the `ddim_p_sample_loop` method,\n",
    "                # use the `p_sample_loop` method instead.\n",
    "                x0 = ddpm.p_sample_loop(shape=(num_vis_particles, 2)).cpu()\n",
    "                # x0 = ddpm.ddim_p_sample_loop(shape=(num_vis_particles, 2)).cpu()\n",
    "                ####\n",
    "                fig, ax = plt.subplots(1,1)\n",
    "                ax.scatter(x0[:,0], x0[:,1])\n",
    "                ax.set_title(f\"Samples at {step}-iteration\")\n",
    "                clear_output(wait=True)\n",
    "                plt.show()\n",
    "                img = figure2image(fig)\n",
    "                images.append(img)\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    if len(images) > 0:\n",
    "        slider = IntSlider(min=0, max=len(images)-1, step=1, value=1)\n",
    "        output = Output()\n",
    "        def display_image(index):\n",
    "            with output:\n",
    "                output.clear_output(wait=True)\n",
    "                display(images[index])\n",
    "        interact(display_image, index=slider)\n",
    "        display(output)\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Loss curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c302ce-4d24-4736-bd27-60d156ba2852",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac2c1bb-eac1-4baf-8d6c-1da319bcce60",
   "metadata": {},
   "source": [
    "## DDPM sampling (Assignment 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5515c0-d109-4ca9-b52a-bd2980542fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will get full credits if your chamfer distance is lower than 20.\n",
    "from chamferdist import chamfer_distance\n",
    "\n",
    "num_eval_particles = 2048\n",
    "pc_ref = target_ds[:num_eval_particles]\n",
    "pc_gen = ddpm.p_sample_loop(shape=(num_eval_particles, 2))\n",
    "\n",
    "pc_gen = pc_gen.reshape(1, num_eval_particles, 2)\n",
    "pc_ref = pc_ref.reshape(1, num_eval_particles, 2)\n",
    "with torch.no_grad():\n",
    "    cd = chamfer_distance(\n",
    "            pc_gen.reshape(-1, 2).cpu().numpy(),\n",
    "            pc_ref.reshape(-1, 2).cpu().numpy(),\n",
    "        )\n",
    "    print(f\"DDPM Chamfer Distance: {cd.item():.4f}\")\n",
    "\n",
    "# Visualize samples with the target distribution.\n",
    "pc_gen = pc_gen.reshape(num_eval_particles, 2).cpu().numpy()\n",
    "pc_ref = pc_ref.reshape(num_eval_particles, 2).cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(pc_ref[:,0], pc_ref[:,1], alpha=0.1, label=\"target distribution\")\n",
    "ax.scatter(pc_gen[:,0], pc_gen[:,1], alpha=0.1, label=\"samples\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs492-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
